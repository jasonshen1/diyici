#!/usr/bin/env python3
"""
çƒ­ç‚¹çŒæ‰‹ - RSSèšåˆå™¨
æ”¶é›†å„å¤§æ–°é—»ç½‘ç«™RSSï¼Œè·å–ç»¼åˆçƒ­ç‚¹
"""

import json
import time
import random
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import List, Dict
import requests
from urllib.parse import urljoin

class RSSAggregator:
    """RSSèšåˆå™¨ - å¤šæºæ–°é—»é‡‡é›†"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # RSSæºé…ç½®
        self.rss_sources = {
            # ç§‘æŠ€ç±»
            '36æ°ª': {
                'url': 'https://rsshub.app/36kr/news/latest',
                'category': 'ç§‘æŠ€',
                'weight': 1.2
            },
            'è™å—…': {
                'url': 'https://rsshub.app/huxiu/article',
                'category': 'ç§‘æŠ€',
                'weight': 1.2
            },
            'å°‘æ•°æ´¾': {
                'url': 'https://rsshub.app/sspai/index',
                'category': 'ç§‘æŠ€',
                'weight': 1.0
            },
            'çˆ±èŒƒå„¿': {
                'url': 'https://rsshub.app/ifanr/app',
                'category': 'ç§‘æŠ€',
                'weight': 1.0
            },
            
            # è´¢ç»ç±»
            'è´¢è”ç¤¾': {
                'url': 'https://rsshub.app/cls/depth',
                'category': 'è´¢ç»',
                'weight': 1.1
            },
            'é›ªçƒ': {
                'url': 'https://rsshub.app/xueqiu/hots',
                'category': 'è´¢ç»',
                'weight': 1.0
            },
            
            # ç»¼åˆæ–°é—»
            'æ¾æ¹ƒæ–°é—»': {
                'url': 'https://rsshub.app/thepaper/featured',
                'category': 'ç¤¾ä¼š',
                'weight': 1.2
            },
            'ç•Œé¢æ–°é—»': {
                'url': 'https://rsshub.app/jiemian/list/71',
                'category': 'ç¤¾ä¼š',
                'weight': 1.0
            },
            
            # å›½é™…
            'BBCä¸­æ–‡': {
                'url': 'https://rsshub.app/bbc/chinese',
                'category': 'å›½é™…',
                'weight': 1.0
            },
            
            # å¨±ä¹/ç”Ÿæ´»
            'è±†ç“£ç”µå½±': {
                'url': 'https://rsshub.app/douban/movie/playing',
                'category': 'å¨±ä¹',
                'weight': 0.9
            },
        }
    
    def fetch_rss(self, name: str, config: dict) -> List[Dict]:
        """
        æŠ“å–å•ä¸ªRSSæº
        """
        try:
            print(f"  ğŸ“¡ {name}...", end=" ")
            
            response = self.session.get(config['url'], timeout=15)
            response.encoding = 'utf-8'
            
            # ä½¿ç”¨æ›´å¥å£®çš„RSSè§£æ
            content = response.text
            
            # å°è¯•è§£æ
            try:
                root = ET.fromstring(response.content)
            except ET.ParseError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æ¸…ç†å†…å®¹
                import re
                # ç§»é™¤éæ³•XMLå­—ç¬¦
                content = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f]', '', content)
                try:
                    root = ET.fromstring(content.encode('utf-8'))
                except:
                    print("âŒ è§£æå¤±è´¥")
                    return []
            
            topics = []
            
            # å°è¯•ä¸åŒæ–¹å¼æ‰¾item
            items = root.findall('.//item')
            if not items:
                # å°è¯•å¸¦å‘½åç©ºé—´çš„
                ns = {'rss': 'http://purl.org/rss/1.0/'}
                items = root.findall('.//rss:item', ns)
            
            for idx, item in enumerate(items[:10], 1):
                try:
                    title_elem = item.find('title')
                    title = title_elem.text.strip() if title_elem is not None and title_elem.text else ''
                    
                    if not title:
                        continue
                    
                    link_elem = item.find('link')
                    link = link_elem.text if link_elem is not None else ''
                    
                    desc_elem = item.find('description')
                    description = ''
                    if desc_elem is not None and desc_elem.text:
                        # æ¸…ç†HTMLæ ‡ç­¾
                        import re
                        description = re.sub(r'<[^>]+>', '', desc_elem.text)
                        description = description[:200]
                    
                    base_hot = 1000000 - (idx * 50000)
                    hot_value = int(base_hot * config['weight'])
                    
                    topics.append({
                        "rank": idx,
                        "title": title,
                        "platform": name,
                        "hot_value": hot_value,
                        "category": config['category'],
                        "url": link,
                        "description": description,
                        "timestamp": datetime.now().isoformat()
                    })
                    
                except Exception as e:
                    continue
            
            print(f"âœ… {len(topics)}æ¡")
            return topics
            
        except Exception as e:
            print(f"âŒ {str(e)[:30]}")
            return []
    
    def fetch_all(self) -> List[Dict]:
        """æŠ“å–æ‰€æœ‰RSSæº"""
        print("ğŸš€ RSSèšåˆå™¨å¯åŠ¨...\n")
        
        all_topics = []
        
        for name, config in self.rss_sources.items():
            topics = self.fetch_rss(name, config)
            if topics:
                all_topics.extend(topics)
            time.sleep(random.uniform(1, 2))  # ç¤¼è²Œå»¶è¿Ÿ
        
        # å»é‡ï¼ˆåŸºäºæ ‡é¢˜å‰20å­—ï¼‰
        seen = set()
        unique = []
        for t in all_topics:
            key = t['title'][:20]
            if key not in seen:
                seen.add(key)
                unique.append(t)
        
        # æŒ‰çƒ­åº¦æ’åº
        unique.sort(key=lambda x: x['hot_value'], reverse=True)
        
        # é‡æ–°ç¼–å·
        for idx, t in enumerate(unique, 1):
            t['rank'] = idx
        
        return unique
    
    def save(self, topics: List[Dict]):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON
        json_file = f"rss_topics_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(topics, f, ensure_ascii=False, indent=2)
        
        # MarkdownæŠ¥å‘Š
        md_file = f"rss_report_{timestamp}.md"
        md = self._generate_markdown(topics)
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md)
        
        return json_file, md_file
    
    def _generate_markdown(self, topics: List[Dict]) -> str:
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        md = f"""# ğŸ“° RSSèšåˆçƒ­ç‚¹æŠ¥å‘Š

> ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}  
> æ•°æ®æ¥æºï¼š{len(self.rss_sources)} ä¸ªRSSæº  
> çƒ­ç‚¹æ€»æ•°ï¼š{len(topics)} æ¡

---

"""
        
        # æŒ‰åˆ†ç±»åˆ†ç»„
        by_cat = {}
        for t in topics:
            cat = t['category']
            if cat not in by_cat:
                by_cat[cat] = []
            by_cat[cat].append(t)
        
        # è¾“å‡ºæ¯ä¸ªåˆ†ç±»
        for cat, items in sorted(by_cat.items(), key=lambda x: -len(x[1])):
            md += f"## ğŸ·ï¸ {cat} ({len(items)}æ¡)\n\n"
            
            for t in items[:15]:  # æ¯ç±»æœ€å¤š15æ¡
                md += f"**{t['rank']}. {t['title']}**\n"
                md += f"- æ¥æºï¼š{t['platform']} | çƒ­åº¦ï¼š{t['hot_value']:,}\n"
                if t.get('description'):
                    md += f"- ç®€ä»‹ï¼š{t['description'][:100]}...\n"
                md += "\n"
        
        md += """---

*æŠ¥å‘Šç”± RSSèšåˆå™¨è‡ªåŠ¨ç”Ÿæˆ*
"""
        return md


def main():
    """ä¸»å‡½æ•°"""
    import random
    
    aggregator = RSSAggregator()
    topics = aggregator.fetch_all()
    
    print("\n" + "="*60)
    print(f"ğŸ“Š æ€»è®¡ï¼š{len(topics)} æ¡çƒ­ç‚¹")
    print("="*60)
    
    # åˆ†ç±»ç»Ÿè®¡
    cats = {}
    for t in topics:
        cat = t['category']
        cats[cat] = cats.get(cat, 0) + 1
    
    print("\nğŸ“ˆ åˆ†ç±»åˆ†å¸ƒï¼š")
    for cat, count in sorted(cats.items(), key=lambda x: -x[1]):
        print(f"  â€¢ {cat}: {count}æ¡")
    
    print("\nğŸ”¥ TOP 20 çƒ­ç‚¹ï¼š")
    print("-"*60)
    for t in topics[:20]:
        emoji = {'ç§‘æŠ€': 'ğŸ’»', 'è´¢ç»': 'ğŸ’°', 'ç¤¾ä¼š': 'ğŸ“°', 
                'å›½é™…': 'ğŸŒ', 'å¨±ä¹': 'ğŸ¬'}.get(t['category'], 'ğŸ“„')
        print(f"{t['rank']:2d}. {emoji} [{t['category']}] {t['title'][:40]}...")
        print(f"    æ¥æºï¼š{t['platform']} | çƒ­åº¦ï¼š{t['hot_value']:,}")
    
    # ä¿å­˜
    json_file, md_file = aggregator.save(topics)
    print(f"\nğŸ’¾ å·²ä¿å­˜ï¼š")
    print(f"  â€¢ {json_file}")
    print(f"  â€¢ {md_file}")


if __name__ == "__main__":
    main()
