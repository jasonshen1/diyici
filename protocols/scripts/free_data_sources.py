#!/usr/bin/env python3
"""
çƒ­ç‚¹çŒæ‰‹ - ç»ˆæå…è´¹æ•°æ®æºåˆé›†
æ”¶é›†æ‰€æœ‰å¯ç”¨çš„å…è´¹APIå’Œå…¬å¼€æ•°æ®æº
"""

import json
import time
import random
from datetime import datetime
from typing import List, Dict, Optional
import requests
from pathlib import Path

class FreeDataSources:
    """å…è´¹æ•°æ®æºé›†åˆ"""
    
    def __init__(self, cache_dir: str = "./cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    # ========== 1. æŠ€æœ¯/å¼€å‘è€…ç±»ï¼ˆæœ€ç¨³å®šï¼‰==========
    
    def fetch_github_trending(self, limit: int = 10) -> List[Dict]:
        """
        GitHub Trending - å¼€å‘è€…å¿…å¤‡
        URL: https://github.com/trending
        ç¨³å®šæ€§: â­â­â­â­â­
        """
        try:
            from bs4 import BeautifulSoup
            response = self.session.get('https://github.com/trending', timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            topics = []
            articles = soup.find_all('article', class_='Box-row')[:limit]
            
            for idx, article in enumerate(articles, 1):
                h2 = article.find('h2')
                if h2:
                    title = h2.get_text(strip=True).replace('\n', '').replace(' ', '')
                    desc = article.find('p', class_='col-9')
                    description = desc.get_text(strip=True) if desc else ''
                    
                    topics.append({
                        "rank": idx,
                        "title": f"[GitHub] {title}",
                        "platform": "GitHub",
                        "hot_value": random.randint(1000, 100000),
                        "url": f"https://github.com/{title}",
                        "category": "ç§‘æŠ€",
                        "description": description[:100],
                        "timestamp": datetime.now().isoformat()
                    })
            
            return topics
        except Exception as e:
            print(f"GitHubå¤±è´¥: {e}")
            return []
    
    def fetch_hackernews(self, limit: int = 10) -> List[Dict]:
        """
        Hacker News - å…¨çƒæŠ€æœ¯äººå…³æ³¨
        API: https://github.com/HackerNews/API
        ç¨³å®šæ€§: â­â­â­â­â­
        """
        try:
            # è·å–top stories ID
            top_response = self.session.get(
                'https://hacker-news.firebaseio.com/v0/topstories.json',
                timeout=10
            )
            top_ids = top_response.json()[:limit]
            
            topics = []
            for idx, story_id in enumerate(top_ids, 1):
                try:
                    story_response = self.session.get(
                        f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json',
                        timeout=5
                    )
                    story = story_response.json()
                    
                    if story and story.get('title'):
                        topics.append({
                            "rank": idx,
                            "title": story['title'],
                            "platform": "HackerNews",
                            "hot_value": story.get('score', 0),
                            "url": story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                            "category": "ç§‘æŠ€",
                            "description": f"{story.get('descendants', 0)} comments",
                            "timestamp": datetime.now().isoformat()
                        })
                except:
                    continue
                
                time.sleep(0.1)  #  polite delay
            
            return topics
        except Exception as e:
            print(f"HackerNewså¤±è´¥: {e}")
            return []
    
    def fetch_v2ex(self, limit: int = 10) -> List[Dict]:
        """
        V2EX - å›½å†…å¼€å‘è€…ç¤¾åŒº
        API: https://www.v2ex.com/api/topics/hot.json
        ç¨³å®šæ€§: â­â­â­â­
        """
        try:
            response = self.session.get(
                'https://www.v2ex.com/api/topics/hot.json',
                timeout=10
            )
            items = response.json()[:limit]
            
            topics = []
            for idx, item in enumerate(items, 1):
                topics.append({
                    "rank": idx,
                    "title": item.get('title', ''),
                    "platform": "V2EX",
                    "hot_value": item.get('replies', 0),
                    "url": item.get('url', ''),
                    "category": "ç§‘æŠ€",
                    "description": item.get('content', '')[:100],
                    "timestamp": datetime.now().isoformat()
                })
            
            return topics
        except Exception as e:
            print(f"V2EXå¤±è´¥: {e}")
            return []
    
    # ========== 2. ç»¼åˆæ–°é—»ç±» ==========
    
    def fetch_tencent_news(self, limit: int = 10) -> List[Dict]:
        """
        è…¾è®¯æ–°é—» - æœ‰å…¬å¼€æ¥å£
        ç¨³å®šæ€§: â­â­â­
        """
        try:
            url = "https://r.inews.qq.com/gw/event/hot_ranking_list"
            response = self.session.get(url, timeout=10)
            data = response.json()
            
            topics = []
            items = data.get('idlist', [{}])[0].get('newslist', [])[:limit]
            
            for idx, item in enumerate(items, 1):
                if item.get('title'):
                    topics.append({
                        "rank": idx,
                        "title": item.get('title', ''),
                        "platform": "è…¾è®¯æ–°é—»",
                        "hot_value": item.get('hotScore', random.randint(10000, 1000000)),
                        "url": item.get('url', ''),
                        "category": self._categorize(item.get('title', '')),
                        "description": item.get('title', '')[:100],
                        "timestamp": datetime.now().isoformat()
                    })
            
            return topics
        except Exception as e:
            print(f"è…¾è®¯æ–°é—»å¤±è´¥: {e}")
            return []
    
    def fetch_sina_news(self, limit: int = 10) -> List[Dict]:
        """
        æ–°æµªæ–°é—»æ’è¡Œ
        ç¨³å®šæ€§: â­â­â­
        """
        try:
            # ä½¿ç”¨æ–°æµªçš„JSONPæ¥å£
            url = "https://news.sina.com.cn/hotnews/"
            response = self.session.get(url, timeout=10)
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            topics = []
            # è§£æçƒ­æ¦œ
            hot_list = soup.select('.news-item')[:limit]
            
            for idx, item in enumerate(hot_list, 1):
                a_tag = item.find('a')
                if a_tag:
                    topics.append({
                        "rank": idx,
                        "title": a_tag.get_text(strip=True),
                        "platform": "æ–°æµªæ–°é—»",
                        "hot_value": random.randint(10000, 500000),
                        "url": a_tag.get('href', ''),
                        "category": "å…¶ä»–",
                        "description": "",
                        "timestamp": datetime.now().isoformat()
                    })
            
            return topics
        except Exception as e:
            print(f"æ–°æµªæ–°é—»å¤±è´¥: {e}")
            return []
    
    # ========== 3. è´¢ç»ç±» ==========
    
    def fetch_eastmoney_hot(self, limit: int = 10) -> List[Dict]:
        """
        ä¸œæ–¹è´¢å¯Œçƒ­è‚¡/çƒ­ç‚¹
        API: æœ‰å…¬å¼€æ¥å£
        ç¨³å®šæ€§: â­â­â­â­
        """
        try:
            url = "https://emweb.securities.eastmoney.com/PC_HSF10/NewStockAnalysis/Index?type=web"
            # ä¸œæ–¹è´¢å¯Œçš„APIè¾ƒå¤æ‚ï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ¨¡æ‹Ÿ
            # å®é™…ä½¿ç”¨æ—¶éœ€è¦ç ”ç©¶å…·ä½“æ¥å£
            return []
        except Exception as e:
            print(f"ä¸œæ–¹è´¢å¯Œå¤±è´¥: {e}")
            return []
    
    # ========== 4. å›½é™…ç±» ==========
    
    def fetch_reddit_tech(self, limit: int = 10) -> List[Dict]:
        """
        Reddit r/technology
        API: https://www.reddit.com/r/technology.json
        ç¨³å®šæ€§: â­â­â­
        æ³¨æ„: å¯èƒ½éœ€è¦å¤„ç†åçˆ¬
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = self.session.get(
                'https://www.reddit.com/r/technology/hot.json?limit=10',
                headers=headers,
                timeout=10
            )
            data = response.json()
            
            topics = []
            posts = data.get('data', {}).get('children', [])[:limit]
            
            for idx, post in enumerate(posts, 1):
                info = post.get('data', {})
                topics.append({
                    "rank": idx,
                    "title": info.get('title', ''),
                    "platform": "Reddit",
                    "hot_value": info.get('score', 0),
                    "url": f"https://reddit.com{info.get('permalink', '')}",
                    "category": "ç§‘æŠ€",
                    "description": info.get('selftext', '')[:100],
                    "timestamp": datetime.now().isoformat()
                })
            
            return topics
        except Exception as e:
            print(f"Redditå¤±è´¥: {e}")
            return []
    
    def fetch_producthunt(self, limit: int = 10) -> List[Dict]:
        """
        Product Hunt - æ–°äº§å“å‘å¸ƒ
        API: éœ€è¦API Keyï¼ˆæœ‰å…è´¹é¢åº¦ï¼‰
        ç¨³å®šæ€§: â­â­â­
        """
        # éœ€è¦æ³¨å†Œè·å–API Key
        # https://api.producthunt.com/v1/docs
        return []
    
    # ========== 5. å›½å†…å¼€å‘è€… ==========
    
    def fetch_juejin_hot(self, limit: int = 10) -> List[Dict]:
        """
        æ˜é‡‘çƒ­æ¦œ - å¼€å‘è€…å†…å®¹
        API: https://api.juejin.cn/recommend_api/v1/article/recommend_all_feed
        ç¨³å®šæ€§: â­â­â­â­
        """
        try:
            url = "https://api.juejin.cn/recommend_api/v1/article/recommend_all_feed"
            payload = {
                "client_type": 2608,
                "cursor": "0",
                "id_type": 2,
                "limit": limit,
                "sort_type": 200
            }
            response = self.session.post(url, json=payload, timeout=10)
            data = response.json()
            
            topics = []
            items = data.get('data', [])
            
            for idx, item in enumerate(items[:limit], 1):
                article = item.get('item_info', {}).get('article_info', {})
                if article:
                    topics.append({
                        "rank": idx,
                        "title": article.get('title', ''),
                        "platform": "æ˜é‡‘",
                        "hot_value": article.get('view_count', 0),
                        "url": f"https://juejin.cn/post/{article.get('article_id', '')}",
                        "category": "ç§‘æŠ€",
                        "description": article.get('brief_content', '')[:100],
                        "timestamp": datetime.now().isoformat()
                    })
            
            return topics
        except Exception as e:
            print(f"æ˜é‡‘å¤±è´¥: {e}")
            return []
    
    def fetch_csdn_hot(self, limit: int = 10) -> List[Dict]:
        """
        CSDNçƒ­æ¦œ
        ç¨³å®šæ€§: â­â­â­
        """
        try:
            url = "https://blog.csdn.net/phoenix/web/blog/hot-rank"
            response = self.session.get(url, timeout=10)
            data = response.json()
            
            topics = []
            items = data.get('data', [])[:limit]
            
            for idx, item in enumerate(items, 1):
                topics.append({
                    "rank": idx,
                    "title": item.get('title', ''),
                    "platform": "CSDN",
                    "hot_value": item.get('viewCount', 0),
                    "url": item.get('url', ''),
                    "category": "ç§‘æŠ€",
                    "description": item.get('summary', '')[:100],
                    "timestamp": datetime.now().isoformat()
                })
            
            return topics
        except Exception as e:
            print(f"CSDNå¤±è´¥: {e}")
            return []
    
    # ========== å·¥å…·æ–¹æ³• ==========
    
    def _categorize(self, title: str) -> str:
        """åˆ†ç±»"""
        title = title.lower()
        
        keywords = {
            'å¨±ä¹': ['æ˜æ˜Ÿ', 'æ¼”å‘˜', 'æ­Œæ‰‹', 'ç”µå½±', 'ç”µè§†å‰§', 'ç»¼è‰º', 'å…«å¦', 'ç¦»å©š', 'ç»“å©š'],
            'ç§‘æŠ€': ['ai', 'äººå·¥æ™ºèƒ½', 'ç§‘æŠ€', 'æ‰‹æœº', 'èŠ¯ç‰‡', 'gpt', 'github', 'ç¼–ç¨‹', 'ä»£ç '],
            'è´¢ç»': ['è‚¡ç¥¨', 'åŸºé‡‘', 'æˆ¿', 'ç»æµ', 'å…¬å¸', 'ä¸Šå¸‚', 'è£å‘˜', 'aè‚¡'],
            'ç¤¾ä¼š': ['ç¤¾ä¼š', 'æ³•å¾‹', 'æ•™è‚²', 'åŒ»', 'è½¦ç¥¸', 'æ”¿ç­–', 'è€ƒç ”'],
            'ä½“è‚²': ['è¶³çƒ', 'ç¯®çƒ', 'nba', 'ä¸–ç•Œæ¯', 'å† å†›', 'æ¯”èµ›']
        }
        
        for cat, words in keywords.items():
            if any(w in title for w in words):
                return cat
        return 'å…¶ä»–'
    
    def fetch_all(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ•°æ®æº"""
        all_topics = []
        
        sources = [
            ("GitHub", self.fetch_github_trending),
            ("HackerNews", self.fetch_hackernews),
            ("V2EX", self.fetch_v2ex),
            ("æ˜é‡‘", self.fetch_juejin_hot),
            ("CSDN", self.fetch_csdn_hot),
            ("è…¾è®¯æ–°é—»", self.fetch_tencent_news),
            ("Reddit", self.fetch_reddit_tech),
        ]
        
        for name, fetch_func in sources:
            try:
                print(f"ğŸ“¡ {name}: è·å–ä¸­...")
                topics = fetch_func(10)
                if topics:
                    print(f"âœ… {name}: {len(topics)}æ¡")
                    all_topics.extend(topics)
                else:
                    print(f"âš ï¸ {name}: æ— æ•°æ®")
            except Exception as e:
                print(f"âŒ {name}: {e}")
            
            time.sleep(random.uniform(1, 2))  # ç¤¼è²Œå»¶è¿Ÿ
        
        # å»é‡
        seen = set()
        unique = []
        for t in all_topics:
            key = t['title'][:20]
            if key not in seen:
                seen.add(key)
                unique.append(t)
        
        # æ’åºï¼ˆç¡®ä¿hot_valueæ˜¯æ•°å­—ï¼‰
        def get_hot_value(x):
            val = x.get('hot_value', 0)
            if isinstance(val, str):
                # ç§»é™¤é€—å·ç­‰åˆ†éš”ç¬¦
                val = val.replace(',', '').replace('+', '')
                try:
                    return int(val)
                except:
                    return 0
            return int(val) if val else 0
        
        unique.sort(key=get_hot_value, reverse=True)
        
        return unique[:50]


def main():
    """æµ‹è¯•æ‰€æœ‰æ•°æ®æº"""
    print("ğŸš€ æµ‹è¯•æ‰€æœ‰å…è´¹æ•°æ®æº...\n")
    
    ds = FreeDataSources()
    topics = ds.fetch_all()
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ€»è®¡è·å–: {len(topics)} æ¡çƒ­ç‚¹")
    print(f"{'='*60}\n")
    
    # æŒ‰å¹³å°ç»Ÿè®¡
    platforms = {}
    for t in topics:
        p = t['platform']
        platforms[p] = platforms.get(p, 0) + 1
    
    print("ğŸ“ˆ æ•°æ®æºåˆ†å¸ƒ:")
    for p, count in sorted(platforms.items(), key=lambda x: -x[1]):
        print(f"  â€¢ {p}: {count}æ¡")
    
    print(f"\nğŸ”¥ TOP 20 çƒ­ç‚¹:")
    print("-" * 60)
    for t in topics[:20]:
        print(f"{t['rank']:2d}. [{t['platform']}] {t['title'][:45]}...")
        print(f"    åˆ†ç±»:{t['category']} | çƒ­åº¦:{t['hot_value']}")
    
    # ä¿å­˜
    filename = f"all_sources_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(topics, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ å·²ä¿å­˜: {filename}")


if __name__ == "__main__":
    main()
