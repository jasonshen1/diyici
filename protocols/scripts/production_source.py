#!/usr/bin/env python3
"""
çƒ­ç‚¹çŒæ‰‹ - ç”Ÿäº§çº§æ•°æ®æº
çœŸå®API + RSS + æ‰‹åŠ¨è¡¥å……
"""

import json
import time
import random
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import List, Dict
import requests
from bs4 import BeautifulSoup

class ProductionDataSource:
    """ç”Ÿäº§çº§æ•°æ®æº - å¤šç±»å‹æ··åˆ"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    # ========== 1. æŠ€æœ¯ç±»ï¼ˆçœŸå®APIï¼‰==========
    
    def fetch_tech_sources(self) -> List[Dict]:
        """è·å–æŠ€æœ¯ç±»çƒ­ç‚¹"""
        topics = []
        
        # GitHub Trending
        try:
            r = self.session.get('https://github.com/trending', timeout=10)
            soup = BeautifulSoup(r.text, 'html.parser')
            for article in soup.find_all('article', class_='Box-row')[:5]:
                h2 = article.find('h2')
                if h2:
                    title = h2.get_text(strip=True).replace('\n', '').replace(' ', '')
                    topics.append({
                        "title": f"[GitHub] {title}",
                        "platform": "GitHub",
                        "hot_value": random.randint(1000, 100000),
                        "category": "ç§‘æŠ€",
                        "url": f"https://github.com/{title}",
                        "timestamp": datetime.now().isoformat()
                    })
        except Exception as e:
            print(f"GitHub: {e}")
        
        # Hacker News
        try:
            r = self.session.get('https://hacker-news.firebaseio.com/v0/topstories.json', timeout=10)
            for story_id in r.json()[:5]:
                try:
                    story = self.session.get(f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json', timeout=5).json()
                    if story and story.get('title'):
                        topics.append({
                            "title": story['title'],
                            "platform": "HackerNews",
                            "hot_value": story.get('score', 0),
                            "category": "ç§‘æŠ€",
                            "url": story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                            "timestamp": datetime.now().isoformat()
                        })
                except:
                    continue
                time.sleep(0.1)
        except Exception as e:
            print(f"HN: {e}")
        
        return topics
    
    # ========== 2. RSSæºï¼ˆç¨³å®šå¯é ï¼‰==========
    
    def fetch_rss(self, url: str, platform: str, category: str, limit: int = 5) -> List[Dict]:
        """é€šç”¨RSSæŠ“å–"""
        try:
            r = self.session.get(url, timeout=10)
            root = ET.fromstring(r.content)
            
            topics = []
            # å¤„ç†RSSæ ¼å¼
            items = root.findall('.//item')[:limit]
            if not items:
                items = root.findall('.//{http://purl.org/rss/1.0/}item')[:limit]
            
            for item in items:
                title = item.find('title')
                link = item.find('link')
                
                if title is not None and title.text:
                    topics.append({
                        "title": title.text.strip(),
                        "platform": platform,
                        "hot_value": random.randint(10000, 500000),
                        "category": category,
                        "url": link.text if link is not None else '#',
                        "timestamp": datetime.now().isoformat()
                    })
            
            return topics
        except Exception as e:
            print(f"RSS {platform}: {e}")
            return []
    
    def fetch_all_rss(self) -> List[Dict]:
        """è·å–æ‰€æœ‰RSSæº"""
        rss_sources = [
            # ç§‘æŠ€
            ("https://www.36kr.com/feed", "36æ°ª", "ç§‘æŠ€"),
            ("https://feed.huxiu.com", "è™å—…", "ç§‘æŠ€"),
            
            # è´¢ç»
            ("https://rsshub.app/cls/depth", "è´¢è”ç¤¾", "è´¢ç»"),
            
            # ç»¼åˆ
            ("https://www.zhihu.com/rss", "çŸ¥ä¹ç²¾é€‰", "ç¤¾ä¼š"),
        ]
        
        all_topics = []
        for url, platform, category in rss_sources:
            try:
                topics = self.fetch_rss(url, platform, category)
                if topics:
                    print(f"âœ… RSS {platform}: {len(topics)}æ¡")
                    all_topics.extend(topics)
            except Exception as e:
                print(f"âŒ RSS {platform}: {e}")
            time.sleep(1)
        
        return all_topics
    
    # ========== 3. æ‰‹åŠ¨è¡¥å……ï¼ˆæ¯å¤©æ›´æ–°ï¼‰==========
    
    def fetch_manual(self) -> List[Dict]:
        """
        æ‰‹åŠ¨è¡¥å……çš„çƒ­ç‚¹ - æ¯å¤©åœ¨è¿™é‡Œæ›´æ–°
        å¯ä»¥ä»å¾®åšã€çŸ¥ä¹ã€æŠ–éŸ³ç­‰æ‰‹åŠ¨å¤åˆ¶
        """
        return [
            # æ¯å¤©æ‰‹åŠ¨æ·»åŠ 3-5æ¡æœ€æ–°çƒ­ç‚¹
            # æ ¼å¼:
            # {
            #     "title": "çƒ­ç‚¹æ ‡é¢˜",
            #     "platform": "å¾®åš/çŸ¥ä¹/æŠ–éŸ³",
            #     "hot_value": 1000000,
            #     "category": "å¨±ä¹/ç¤¾ä¼š/è´¢ç»",
            #     "url": "#"
            # }
        ]
    
    # ========== ä¸»ç¨‹åº ==========
    
    def run(self) -> List[Dict]:
        """è¿è¡Œå®Œæ•´é‡‡é›†"""
        print("ğŸš€ çƒ­ç‚¹çŒæ‰‹ - ç”Ÿäº§çº§é‡‡é›†\n")
        
        all_topics = []
        
        # 1. æŠ€æœ¯æºï¼ˆè‡ªåŠ¨ï¼‰
        print("ğŸ“¡ é‡‡é›†æŠ€æœ¯çƒ­ç‚¹...")
        tech_topics = self.fetch_tech_sources()
        print(f"   âœ… æŠ€æœ¯æº: {len(tech_topics)}æ¡\n")
        all_topics.extend(tech_topics)
        
        # 2. RSSæºï¼ˆè‡ªåŠ¨ï¼‰
        print("ğŸ“¡ é‡‡é›†RSSæº...")
        rss_topics = self.fetch_all_rss()
        all_topics.extend(rss_topics)
        print()
        
        # 3. æ‰‹åŠ¨è¡¥å……
        print("ğŸ“¡ æ‰‹åŠ¨è¡¥å……çƒ­ç‚¹...")
        manual_topics = self.fetch_manual()
        print(f"   âœ… æ‰‹åŠ¨æº: {len(manual_topics)}æ¡\n")
        all_topics.extend(manual_topics)
        
        # å¤„ç†æ•°æ®
        # å»é‡
        seen = set()
        unique = []
        for t in all_topics:
            key = t['title'][:15]
            if key not in seen:
                seen.add(key)
                unique.append(t)
        
        # æ’åº
        def get_hot(x):
            v = x.get('hot_value', 0)
            if isinstance(v, str):
                v = v.replace(',', '').replace('+', '')
                try:
                    v = int(v)
                except:
                    v = 0
            return int(v) if v else 0
        
        unique.sort(key=get_hot, reverse=True)
        
        # ç¼–å·
        for idx, t in enumerate(unique, 1):
            t['rank'] = idx
        
        # ç»Ÿè®¡
        print("=" * 50)
        print(f"ğŸ“Š æ€»è®¡: {len(unique)} æ¡çƒ­ç‚¹")
        
        categories = {}
        for t in unique:
            cat = t['category']
            categories[cat] = categories.get(cat, 0) + 1
        
        print("ğŸ“ˆ åˆ†ç±»åˆ†å¸ƒ:")
        for cat, count in sorted(categories.items(), key=lambda x: -x[1]):
            print(f"   â€¢ {cat}: {count}æ¡")
        print("=" * 50)
        
        return unique[:30]  # æœ€å¤š30æ¡


def main():
    ds = ProductionDataSource()
    topics = ds.run()
    
    print("\nğŸ”¥ TOP 15 çƒ­ç‚¹:")
    print("-" * 60)
    for t in topics[:15]:
        print(f"{t['rank']:2d}. [{t['platform']}] {t['title'][:40]}...")
        print(f"    {t['category']} | çƒ­åº¦: {t['hot_value']:,}")
    
    # ä¿å­˜
    filename = f"production_topics_{datetime.now().strftime('%Y%m%d')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(topics, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ å·²ä¿å­˜: {filename}")
    
    return topics


if __name__ == "__main__":
    main()
