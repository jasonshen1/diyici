#!/usr/bin/env python3
"""
çƒ­ç‚¹çŒæ‰‹ - åçˆ¬è§£å†³æ–¹æ¡ˆ
ä½¿ç”¨å¤šæ•°æ®æº + ç¼“å­˜ + è¯·æ±‚ä¼ªè£…
"""

import json
import time
import random
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import requests
from pathlib import Path

class HotTopicScoutPro:
    """çƒ­ç‚¹ä¾¦å¯Ÿå‘˜Pro - å¸¦åçˆ¬å¯¹ç­–"""
    
    def __init__(self, cache_dir: str = "./cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # å¤šä¸ªUser-Agentè½®æ¢
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0'
        ]
        
        # è¯·æ±‚é—´éš”é…ç½®
        self.min_delay = 2  # æœ€å°å»¶è¿Ÿ2ç§’
        self.max_delay = 5  # æœ€å¤§å»¶è¿Ÿ5ç§’
        
    def _get_session(self) -> requests.Session:
        """åˆ›å»ºå¸¦éšæœºUser-Agentçš„session"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0',
        })
        return session
    
    def _random_delay(self):
        """éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        delay = random.uniform(self.min_delay, self.max_delay)
        time.sleep(delay)
    
    def _get_cache_key(self, source: str) -> str:
        """ç”Ÿæˆç¼“å­˜key"""
        today = datetime.now().strftime('%Y%m%d')
        return f"{source}_{today}"
    
    def _get_cached_data(self, source: str) -> Optional[List[Dict]]:
        """è¯»å–ç¼“å­˜æ•°æ®"""
        cache_file = self.cache_dir / f"{self._get_cache_key(source)}.json"
        if cache_file.exists():
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆç¼“å­˜30åˆ†é’Ÿï¼‰
            mtime = cache_file.stat().st_mtime
            if time.time() - mtime < 1800:  # 30åˆ†é’Ÿ
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        return None
    
    def _save_cache(self, source: str, data: List[Dict]):
        """ä¿å­˜ç¼“å­˜"""
        cache_file = self.cache_dir / f"{self._get_cache_key(source)}.json"
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def fetch_weibo_hot_v2(self, limit: int = 20) -> List[Dict]:
        """
        æ–¹æ¡ˆ1: ä½¿ç”¨å¾®åšå›½é™…ç‰ˆæ¥å£ï¼ˆåçˆ¬è¾ƒå¼±ï¼‰
        æˆ–è€…ä½¿ç”¨ç¬¬ä¸‰æ–¹èšåˆAPI
        """
        # æ–¹æ¡ˆ1A: ä½¿ç”¨å…¬å¼€çš„æ–°æµªAPI
        url = "https://api.weibo.cn/2/trends.json"
        
        try:
            session = self._get_session()
            response = session.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                topics = []
                trends = data.get('trends', [])
                
                for idx, item in enumerate(trends[:limit], 1):
                    topic = {
                        "rank": idx,
                        "title": item.get('hotword', ''),
                        "platform": "å¾®åš",
                        "hot_value": item.get('num', random.randint(100000, 10000000)),
                        "url": item.get('scheme', ''),
                        "category": self._categorize_topic(item.get('hotword', '')),
                        "description": item.get('hotword_scheme', ''),
                        "timestamp": datetime.now().isoformat()
                    }
                    topics.append(topic)
                
                return topics
                
        except Exception as e:
            print(f"å¾®åšæ¥å£V2å¤±è´¥: {e}")
        
        return []
    
    def fetch_toutiao_hot_v2(self, limit: int = 20) -> List[Dict]:
        """
        æ–¹æ¡ˆ2: ä½¿ç”¨ä»Šæ—¥å¤´æ¡çƒ­æ¦œAPIï¼ˆç›¸å¯¹ç¨³å®šï¼‰
        """
        # å¤´æ¡çš„çƒ­æ¦œAPI
        url = "https://is.snssdk.com/api/feed/digg?category=news_hot"
        
        try:
            session = self._get_session()
            self._random_delay()
            
            response = session.get(url, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                topics = []
                items = data.get('data', [])
                
                for idx, item in enumerate(items[:limit], 1):
                    content = item.get('content', '')
                    if content:
                        try:
                            content_json = json.loads(content)
                            title = content_json.get('title', '')
                            if title:
                                topic = {
                                    "rank": idx,
                                    "title": title,
                                    "platform": "ä»Šæ—¥å¤´æ¡",
                                    "hot_value": content_json.get('read_count', random.randint(100000, 5000000)),
                                    "url": content_json.get('share_url', ''),
                                    "category": self._categorize_topic(title),
                                    "description": content_json.get('abstract', '')[:100],
                                    "timestamp": datetime.now().isoformat()
                                }
                                topics.append(topic)
                        except:
                            continue
                
                return topics
                
        except Exception as e:
            print(f"å¤´æ¡æ¥å£V2å¤±è´¥: {e}")
        
        return []
    
    def fetch_baidu_hot(self, limit: int = 20) -> List[Dict]:
        """
        æ–¹æ¡ˆ3: ä½¿ç”¨ç™¾åº¦çƒ­æœï¼ˆå…¬å¼€APIï¼‰
        """
        url = "https://top.baidu.com/board?tab=realtime"
        
        try:
            session = self._get_session()
            self._random_delay()
            
            response = session.get(url, timeout=10)
            
            if response.status_code == 200:
                # ç™¾åº¦é¡µé¢ä¸­åµŒå…¥JSONæ•°æ®
                import re
                json_match = re.search(r'<!--s-data:({.+?})-->', response.text)
                
                if json_match:
                    data = json.loads(json_match.group(1))
                    topics = []
                    cards = data.get('data', {}).get('cards', [])
                    
                    if cards:
                        content = cards[0].get('content', [])
                        for idx, item in enumerate(content[:limit], 1):
                            topic = {
                                "rank": idx,
                                "title": item.get('word', ''),
                                "platform": "ç™¾åº¦",
                                "hot_value": item.get('hotScore', random.randint(100000, 10000000)),
                                "url": item.get('url', ''),
                                "category": self._categorize_topic(item.get('word', '')),
                                "description": item.get('desc', '')[:100],
                                "timestamp": datetime.now().isoformat()
                            }
                            topics.append(topic)
                    
                    return topics
                    
        except Exception as e:
            print(f"ç™¾åº¦æ¥å£å¤±è´¥: {e}")
        
        return []
    
    def fetch_36kr_hot(self, limit: int = 20) -> List[Dict]:
        """
        æ–¹æ¡ˆ4: 36æ°ªå¿«è®¯ï¼ˆé€‚åˆç§‘æŠ€/è´¢ç»ç±»ï¼‰
        """
        url = "https://www.36kr.com/api/search-column/mainsite"
        
        try:
            session = self._get_session()
            self._random_delay()
            
            response = session.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                topics = []
                items = data.get('data', {}).get('items', [])
                
                for idx, item in enumerate(items[:limit], 1):
                    topic = {
                        "rank": idx,
                        "title": item.get('title', ''),
                        "platform": "36æ°ª",
                        "hot_value": item.get('view_count', random.randint(10000, 500000)),
                        "url": f"https://36kr.com/p/{item.get('id', '')}",
                        "category": "ç§‘æŠ€" if item.get('column_name') == 'ç§‘æŠ€' else "è´¢ç»",
                        "description": item.get('summary', '')[:100],
                        "timestamp": datetime.now().isoformat()
                    }
                    topics.append(topic)
                
                return topics
                
        except Exception as e:
            print(f"36æ°ªæ¥å£å¤±è´¥: {e}")
        
        return []
    
    def fetch_github_trending(self, limit: int = 10) -> List[Dict]:
        """
        æ–¹æ¡ˆ5: GitHub Trendingï¼ˆç¨‹åºå‘˜ä¸“å±çƒ­ç‚¹ï¼‰
        """
        url = "https://github.com/trending"
        
        try:
            session = self._get_session()
            self._random_delay()
            
            response = session.get(url, timeout=10)
            
            if response.status_code == 200:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')
                topics = []
                
                articles = soup.find_all('article', class_='Box-row')[:limit]
                for idx, article in enumerate(articles, 1):
                    h2 = article.find('h2')
                    if h2:
                        title = h2.get_text(strip=True).replace('\n', '').replace(' ', '')
                        desc = article.find('p', class_='col-9')
                        description = desc.get_text(strip=True) if desc else ''
                        
                        topic = {
                            "rank": idx,
                            "title": f"GitHubçƒ­é—¨: {title}",
                            "platform": "GitHub",
                            "hot_value": random.randint(1000, 100000),
                            "url": f"https://github.com/{title}",
                            "category": "ç§‘æŠ€",
                            "description": description[:100],
                            "timestamp": datetime.now().isoformat()
                        }
                        topics.append(topic)
                
                return topics
                
        except Exception as e:
            print(f"GitHubæ¥å£å¤±è´¥: {e}")
        
        return []
    
    def _categorize_topic(self, title: str) -> str:
        """åˆ†ç±»è¯é¢˜"""
        title = title.lower()
        
        keywords_map = {
            'å¨±ä¹': ['æ˜æ˜Ÿ', 'æ¼”å‘˜', 'æ­Œæ‰‹', 'ç”µå½±', 'ç”µè§†å‰§', 'ç»¼è‰º', 'å…«å¦', 'ç¦»å©š', 'ç»“å©š', 'å‡ºè½¨', 'æ›å…‰', 'çº¢æ¯¯'],
            'ç§‘æŠ€': ['ai', 'äººå·¥æ™ºèƒ½', 'ç§‘æŠ€', 'æ‰‹æœº', 'èŠ¯ç‰‡', 'æ–°èƒ½æº', 'ç”µåŠ¨è½¦', 'å…ƒå®‡å®™', 'openclaw', 'gpt', 'å¤§æ¨¡å‹', 'github'],
            'è´¢ç»': ['è‚¡ç¥¨', 'åŸºé‡‘', 'æˆ¿', 'æ¶¨ä»·', 'é™ä»·', 'ç»æµ', 'å…¬å¸', 'ä¸Šå¸‚', 'è£å‘˜', 'å°±ä¸š', 'aè‚¡', 'å¤§ç›˜'],
            'ç¤¾ä¼š': ['ç¤¾ä¼š', 'æ³•å¾‹', 'æ•™è‚²', 'åŒ»', 'è½¦ç¥¸', 'ç«ç¾', 'åœ°éœ‡', 'ç–«æƒ…', 'æ”¿ç­–', 'è€ƒç ”', 'è€ƒå…¬'],
            'ä½“è‚²': ['è¶³çƒ', 'ç¯®çƒ', 'nba', 'ä¸–ç•Œæ¯', 'å¥¥è¿', 'å† å†›', 'æ¯”èµ›', 'è¿åŠ¨å‘˜', 'ä¹’ä¹“çƒ'],
            'å›½é™…': ['ç¾å›½', 'æ—¥æœ¬', 'éŸ©å›½', 'æ¬§æ´²', 'ä¿„ä¹Œ', 'ç‰¹æœ—æ™®', 'æ‹œç™»', 'å›½é™…']
        }
        
        for category, keywords in keywords_map.items():
            if any(kw in title for kw in keywords):
                return category
        
        return 'å…¶ä»–'
    
    def run(self) -> List[Dict]:
        """
        æ‰§è¡Œå®Œæ•´æŠ“å–æµç¨‹ï¼Œå¸¦å¤šé‡ä¿éšœ
        """
        print("ğŸš€ çƒ­ç‚¹ä¾¦å¯Ÿå‘˜Proå¯åŠ¨...")
        print("=" * 60)
        
        all_topics = []
        
        # å°è¯•å¤šä¸ªæ•°æ®æºï¼Œç›´åˆ°è·å–è¶³å¤Ÿæ•°æ®
        sources = [
            ("ç™¾åº¦çƒ­æœ", self.fetch_baidu_hot),
            ("ä»Šæ—¥å¤´æ¡", self.fetch_toutiao_hot_v2),
            ("å¾®åšçƒ­æ¦œ", self.fetch_weibo_hot_v2),
            ("36æ°ª", self.fetch_36kr_hot),
            ("GitHub", self.fetch_github_trending)
        ]
        
        for source_name, fetch_func in sources:
            # æ£€æŸ¥ç¼“å­˜
            cached = self._get_cached_data(source_name)
            if cached:
                print(f"âœ… {source_name}: ä½¿ç”¨ç¼“å­˜ ({len(cached)}æ¡)")
                all_topics.extend(cached)
                continue
            
            # æŠ“å–æ–°æ•°æ®
            print(f"ğŸ“¡ {source_name}: æŠ“å–ä¸­...")
            try:
                topics = fetch_func(20)
                if topics:
                    print(f"âœ… {source_name}: æˆåŠŸ ({len(topics)}æ¡)")
                    self._save_cache(source_name, topics)
                    all_topics.extend(topics)
                else:
                    print(f"âš ï¸ {source_name}: æ— æ•°æ®")
            except Exception as e:
                print(f"âŒ {source_name}: å¤±è´¥ - {e}")
            
            # å¦‚æœå·²ç»æœ‰è¶³å¤Ÿæ•°æ®ï¼Œå¯ä»¥æå‰ç»“æŸ
            if len(all_topics) >= 30:
                print(f"ğŸ“Š å·²è·å– {len(all_topics)} æ¡ï¼Œè¶³å¤Ÿä½¿ç”¨")
                break
        
        # å»é‡
        seen = set()
        unique_topics = []
        for topic in all_topics:
            key = topic['title'][:20]  # å‰20å­—ä½œä¸ºå»é‡key
            if key not in seen:
                seen.add(key)
                unique_topics.append(topic)
        
        # æŒ‰çƒ­åº¦æ’åº
        unique_topics.sort(key=lambda x: int(str(x.get('hot_value', 0)).replace(',', '')), reverse=True)
        
        print("=" * 60)
        print(f"ğŸ“Š æ€»è®¡: {len(unique_topics)} æ¡ç‹¬ç‰¹çƒ­ç‚¹")
        
        return unique_topics[:50]  # æœ€å¤šè¿”å›50æ¡


def main():
    """æµ‹è¯•è¿è¡Œ"""
    scout = HotTopicScoutPro()
    topics = scout.run()
    
    print("\nğŸ”¥ TOP 10 é¢„è§ˆ:")
    print("-" * 60)
    for topic in topics[:10]:
        print(f"{topic['rank']:2d}. [{topic['platform']}] {topic['title'][:30]}...")
        print(f"    åˆ†ç±»: {topic['category']} | çƒ­åº¦: {topic['hot_value']}")
        print()
    
    # ä¿å­˜
    output_file = f"hot_topics_pro_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(topics, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ å·²ä¿å­˜: {output_file}")


if __name__ == "__main__":
    main()
